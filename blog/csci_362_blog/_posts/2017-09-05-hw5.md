---
layout: post
title: "HW5: Reflections"
description: A response to various readings regarding software system failures without appropriate backup protections.
date: 2017-09-05 10:00:00 -5EST
category: csci_362_blog
---

## **Readings**
* <a href="http://stono.cs.cofc.edu/~bowring/classes/csci%20362/docs/Therac25Accidents.html" target="_blank"><em>An Investigation of the Therac-25 Accidents</em></a>
* <a href="http://stono.cs.cofc.edu/~bowring/classes/csci%20362/docs/The%20Radiation%20Boom%20-%20After%20Stroke%20Scans,%20Patients%20Face%20Serious%20Health%20Risks%20-%20NYTimes.com.pdf" target="_blank"><em>After Stroke Scans, Patients Face Serious
Health Risks</em></a>
* <a href="https://www.ic3.gov/media/2016/160317.aspx" target="_blank"><em>Motor Vehicles Increasingly Vulnerable to Remote Exploits</em></a>
* <a href="http://stono.cs.cofc.edu/~bowring/classes/csci%20362/docs/levesonSoftwareAccidentsSpacecraft.pdf" target="_blank"><em>The Role of Software in Spacecraft Accidents</em></a>
* <a href="http://stono.cs.cofc.edu/~bowring/classes/csci%20362/docs/SpectrumFBIcaseFileSytem.pdf" target="_blank"><em>Who Killed the Virtual Case File?</em></a>
* <a href="http://www.washingtonpost.com/wp-dyn/content/article/2010/10/20/AR2010102006754.html" target="_blank"><em>FBI Sentinel project is over budget and behind schedule, say IG auditors</em></a>
* <a href="https://www.pcmag.com/article2/0,2817,2407922,00.asp" target="_blank"><em>Years Late and Millions Over Budget, FBI's Sentinel Finally On Line</em></a>
* <a href="https://spectrum.ieee.org/riskfactor/computing/it/fbis-500-million-sentinel-case-management-system-still-has-major-operational-kinks-ig-reports" target="_blank"><em>FBIâ€™s Sentinel System Still Not In Total Shape to Surveil</em></a>
* Chapters 13 & 14 of <a href="http://iansommerville.com/software-engineering-book/" target="_blank"><em>Software Engineering 10th Edition</em> by Ian Sommerville</a>

## **Response**
&nbsp;&nbsp;&nbsp;&nbsp;It seems that when we want a piece of software, we have the highest expectations for it, yet the poorest motivation to achieve them. We tend to put together these grandiose ideas of what we want in our minds, expecting robust, cutting-edge technology and functionality at a low cost and without failures. Yet, when the "honeymoon phase" of a new software idea comes crashing down (and it does hard), we fail to provide the effort and tools necessary to achieve what we first set out to build. It's not the fault of any one person, or even group of people. Technology failures occur at all of the various stages of the engineering process.

&nbsp;&nbsp;&nbsp;&nbsp;Even from the earliest designs of our systems, we fail to adequately account for use cases, exceptional cases, and the potential vulnerabilities of those cases. Consider the CT scanner incidents circa 2010. The self-adjusting dosage feature seems, in theory, like a wonderfully useful tool to prevent human error in setting the dosage for patients. In practice however, the feature was implemented without proper training procedures for users of the devices, and as it seems, even if proper training had been given, the feature did not work as intended. In the case of Therac-25, the use transition to software-based fail-safe systems resulted in critical errors and failures to protect patients from high dosages of radiation. While it might seem like a good idea in the design phase, in practice, many obvious dangers were not considered.

&nbsp;&nbsp;&nbsp;&nbsp;If a software has not already been irreparably faulted by the designs of its creators, then it faces the damage surely to come from its implementation. In this pahse, so many critical decisions are made that can potentially destroy key functionality, make the system virtually unusable, or leave critical vulnerabilities open for attackers. In the FBI's Sentinel system, it's clear that while case search and index features were well thought-out, their implementation failed miserably to achieve the specified requirements. Or, in the case of modern car vulnerabilities, many features are provided that drastically improve the driver's experience, but they were developed without considering the massive vulnerabilities of channeled communications between the cars' software and centralized systems.

&nbsp;&nbsp;&nbsp;&nbsp;In an ideal world, we would be able to build software exactly how we design it, on time, on budget, with no bugs, perfect testing, and plentiful fail-safe protections. In reality though, this is simply against our nature as humans. We're naturally disorganized and often incapable of articulating, let alone producing, exactly what we're looking for. And at the same time, our tendency to find the path of least resistance to a solution often finds us cutting corners, skipping essential steps, and covering our butts when it all blows up in our faces. Our personal flaws are encapsulated in our work, and as a result, the software is as imperfect as we are.

&nbsp;&nbsp;&nbsp;&nbsp;Even in an ideal world where architects, engineers, and developers are able to build their ideal systems, it's almost certainly not going to happen on time or on budget. This doesn't just apply to software. When we humans try to take on such monumentally complex tasks, we tend to fail to consider the true cost, time, and labor needed. The FBI's VCF and Sentinel systems are obvious victims of this, though I don't doubt that all of the other cases also suffered the same fate. We set budgetary goals and timelines that are often unrealistic because we don't have a feasible way to properly estimate these things. And even if we could accurately devise an estimate for budget and time, you can never predict the many failures and setbacks that are almost certain to arise in the process.

&nbsp;&nbsp;&nbsp;&nbsp;In the end, there will always be deeply flawed software, because human nature encourages those flaws to exist. In a world with such rapid growth of technology, we often rush to build new systems with unrealistic goals. And when deadlines come and go, we cheat - cutting functionality, testing, reviews, backups, etc. But as software engineers, we owe it to ourselves, our clients, and the general population to strive to build better software and to discourage cheating.
